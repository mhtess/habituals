// time webppl habituals.wppl --require mht --require tjUtils
// time ~/webppl-paul/webppl habituals.wppl --require mht --require tjUtils
var bin_data = mht.readCSV(fpath2+"prior2-discrete_bins.csv").data
var align2afc = {
	"agree-key":"habitual",
	"disagree-key": "mm"
}


var df_bins = _.object(map(function(lst){
	var statebins = _.sortBy(_.values(_.omit(lst, "Item"))) // grab and sort state bins
	var bin_width = statebins[1] - statebins[0] // calculate bin_width based on 1st and 2nd bins (could produce errors if bins are not close to equally spaced)
	return [lst["Item"], 
		{"state_bins": statebins,
		"theta_bins": map(function(x){
			return Math.round(10*(x - (bin_width / 2))) / 10 // get intermediate points (rounded to nearest 0.1)
		},statebins)
		}
		]
}, dataFrame(bin_data, ["0","1","2","3","4","5","6","7","8","9","10"])))


var df_tj_wRounded = map(function(x){
	var i = x["habitual"]
	// var statebins = df_bins[i]["state_bins"]
	// Expt 3 (Figure 3)
	// return _.extend(x, {roundedFreq: nearestPriorBin(x["past_logfreq"], statebins),
	// 					alignedResponse : align2afc[x.response]})
	// return _.extend(x, {roundedFreq: nearestPriorBin(x["future_logfreq"], statebins),
						// alignedResponse : align2afc[x.response]})
	// Expt 2 (Figure 2)
	return _.extend(x, {roundedFreq: nearestPriorBin(x["log_times"], priorBins),
						alignedResponse : align2afc[x.response]})
}, df_tj)

// console.log(df_tj_wRounded)
// console.log(items)
// console.log(subset(df_tj_wRounded, "habitual", item))
// console.log(df_bins[item])


// console.log(df_w)

// var prior_samples = 100000
// var prior_burn = prior_samples/2
// var priorERP = IncrementalMH(priorModel, prior_samples, 
// 	{burn:prior_burn, verbose:true, verboseLag: prior_burn/10}
// 	)

var filepath = "/Users/mht/Documents/research/habituals/models/priors/results/byItem/10k/"
var priorERPobject = _.object(map(function(i){
	console.log('loading prior ERP for ' + i)
	var item = i.split(' ').join('')
	return [i, _.object(map(function(gen){
		return [gen, _.object(map(function(dm){
			var fpath = dm == "existence" ? 
					"priors2"+gen+"_"+item+"-existenceQ-betaModel-incrMH10k_burn5ka.csv" :
					"waitQ-"+gen+"_"+item+"-incrMH10k_burn5ka.csv"
			var df = readQueryERP(filepath+fpath)
			return [dm, df]
		}, ["existence", "wait"]))]
	},genders))]
},items))


// var priorERP = MCMC(priorModel, {
// 	kernel:"HMC",
// 	samples: prior_samples/2, 
// 	burn:prior_burn, 
// 	verbose:true
// })

// var priorERPobject = _.object(map(function(i){
// 	return [i, _.object(map(function(g){
// 		return [g, _.object(map(function(q){
// 			return [q, 	marginalizeERP(priorERP, [i,g,q])]
// 		},["Q1","Q2"]))]
// 	}, genders))]
// }, items))

// console.log('prior marginalized')


var item_bins = {
	state_bins: _.range(-1, 9, 0.5),
	theta_bins: _.range(-0.75,8.75,0.5)
}

// priorERPobject["writes poems"]["male"]["Q2"]; // freq params.
// pp_m
// priorERPobject
// item_bins
var model = function(){

	var speaker1_optimality = uniform(0,20)
	var speaker2_optimality = uniform(0,20)
	// var phi = uniform(0,1)
	var phi = 0
	var prop_male = 0.5
	// var prop_male = uniform(0,1)

	// console.log("so " + speaker_optimality)
	// console.log("phi " + phi)

	foreach(items, function(i){


		var pr_exist_m_param = sample(priorERPobject[i]["male"]["existence"]); //exist params
		var pr_exist_m = beta(shape_alpha(pr_exist_m_param.Gamma, pr_exist_m_param.Delta),
				  			shape_beta(pr_exist_m_param.Gamma, pr_exist_m_param.Delta))

		var pr_exist_f_param = sample(priorERPobject[i]["female"]["existence"]);
		var pr_exist_f = beta(shape_alpha(pr_exist_f_param.Gamma, pr_exist_f_param.Delta),
					  			shape_beta(pr_exist_f_param.Gamma, pr_exist_f_param.Delta))

		var pp_m = sample(priorERPobject[i]["male"]["wait"]); // freq params.
		var pp_f = sample(priorERPobject[i]["female"]["wait"]);

		console.log(pr_exist_m)
		console.log(pr_exist_f)
		console.log(pp_m)
		console.log(pp_f)
		var prior = mix2GaussiansWithDelta(
			prop_male, 
			pr_exist_m, 
			pr_exist_f,
			pp_m.Mu, 
			pp_m.Sigma, 
			pp_f.Mu, 
			pp_f.Sigma, 
			item_bins["state_bins"]
			);


		var itemData = subset(df_tj_wRounded, "habitual", i)
		var freqLevels = _.uniq(_.pluck(itemData, "roundedFreq"))

		// console.log("e = " + pr_exist)
		// console.log("pp = " + pp.join(' '))

		// EXPERIMENT 2
		foreach(freqLevels, function(f){

			var freqData = subset(itemData, "roundedFreq", f)
			var responseData = _.pluck(freqData, "alignedResponse")

			var grossLevel = freqData[0]["time_period"]
			
			var s2 = speaker2(f, prior, speaker1_optimality, speaker2_optimality, item_bins["theta_bins"])
			// var s2 = speaker2(f, prior, speaker_optimality, item_bins["theta_bins"])
			// var s2_plusGuess = guessingLink(s2, phi)

			var scr = reduce(function(response, memo) {
							    return memo + s2.score([], response)
								}, 0, responseData)

			// console.log(i + f + scr)
			// console.log("S2 prob = " + Math.exp(s2.score([], "habitual")))
			factor(scr)

			// var L1 = listener1("habitual", prior, speaker_optimality, item_bins["theta_bins"])
			// print L1 posterior
			// foreach(L1.support(), function(s){
			// 	query.add([s, i, f, grossLevel], Math.exp(L1.score([], s)))
			// })

			// query.add(["L1_predictive", i, f, grossLevel], sample(L1))
			query.add(["predictive", i, f, grossLevel], Math.exp(s2.score([], "habitual")))
		})


		// EXPERIMENT 3
		// foreach(["baseline","preventative","enabling"], function(c){
		// 	// console.log(c)
		// 	var freqData = subset(itemData, "condition", c)
		// 	// console.log(freqData)
		// 	var f = _.pluck(freqData,"roundedFreq")[0]
		// 	var responseData = _.pluck(freqData, "alignedResponse")
		// 	// console.log(f)
		// 	var s2 = speaker2(f, prior, speaker_optimality, item_bins["theta_bins"])
		// 	var s2_plusGuess = guessingLink(s2, phi)
		// 	// console.log(f + i + c + Math.exp(s2.score([], "habitual")))

		// 	var scr = reduce(function(response, memo) {
		// 					    return memo + s2_plusGuess.score([], response)
		// 						}, 0, responseData)

		// 	// console.log(i + f + scr)
		// 	// console.log("S2 prob = " + Math.exp(s2_plusGuess.score([], "habitual")))
		// 	factor(scr)
		// 	query.add(["predictive", i, f, c], Math.exp(s2_plusGuess.score([], "habitual")))

		// })

	})

	query.add(["parameter", "global", "speaker1_optimality", "NA"], speaker1_optimality)
	query.add(["parameter", "global", "speaker2_optimality", "NA"], speaker2_optimality)
	// query.add(["parameter", "global", "phi", "NA"], phi)
	// query.add(["parameter", "global", "prop_male", "NA"], prop_male)
	return query
}

var samples = 500
var burn = samples/2
var resultsERP = IncrementalMH(model, samples, {verbose: true, verboseLag: samples/20, burn: burn})
// var resultsERP = MCMC(model, {
// 	samples: samples/2, 
// 	verbose: true, 
// 	verboseLag: samples/20, 
// 	burn: burn,
// 	kernel: { HMC: { steps: 1, stepSize: 1 }}
// })

var outputFile = "results/tj2-RSA-log_ntimes-corrected2_2so-IncrMH" + 
	samples/1000 +"k_burn" + burn/1000 +"k_prior-mixGenders0.5-"+ 
	prior_samples/1000 + "k_burn" + prior_burn/1000 + "k_discretize-1-8.5-0.5-a.csv"
// var outputFile = "results/HMCtest-tj3-RSA-future_log_ntimes-so-IncrMH" + samples/1000 +"k_burn" + burn/1000 +"k_prior-mixGenders0.5-"+ prior_samples/1000 + 
// 	"k_burn" + prior_burn/1000 + "k_discretize-1-8.5-0.5-a.csv"
var header = "Type,Item,Level,Period,Value"

tjUtils.erpWriter(resultsERP, outputFile, header)
console.log("written to " + outputFile)

